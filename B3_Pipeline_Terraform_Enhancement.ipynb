{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4096a57d",
   "metadata": {},
   "source": [
    "# B3 Pipeline Terraform Enhancement\n",
    "\n",
    "## Requisitos de Melhorias na Infraestrutura\n",
    "\n",
    "Este notebook demonstra como aprimorar os scripts Terraform para atender aos seguintes requisitos:\n",
    "\n",
    "**Requisito 7**: O job Glue deve automaticamente catalogar dados no Glue Catalog e criar tabelas no banco de dados default do Glue Catalog.\n",
    "\n",
    "**Requisito 8**: Os dados devem estar disponíveis e legíveis no Athena.\n",
    "\n",
    "**Requisito 9**: Construir um notebook para visualização gráfica dos dados ingeridos.\n",
    "\n",
    "## Objetivos\n",
    "- ✅ Configurar Glue Catalog automático\n",
    "- ✅ Integrar dados com Amazon Athena  \n",
    "- ✅ Criar visualizações e monitoramento\n",
    "- ✅ Implementar melhores práticas de IaC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057ad932",
   "metadata": {},
   "source": [
    "## 1. Setup AWS Provider and Variables\n",
    "\n",
    "Primeiro, vamos configurar o provider AWS e definir as variáveis necessárias para nossa infraestrutura aprimorada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49de4009",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile terraform/enhanced_main.tf\n",
    "# Enhanced Terraform configuration for B3 Pipeline\n",
    "\n",
    "terraform {\n",
    "  required_providers {\n",
    "    aws = {\n",
    "      source  = \"hashicorp/aws\"\n",
    "      version = \"~> 5.0\"\n",
    "    }\n",
    "    random = {\n",
    "      source  = \"hashicorp/random\"\n",
    "      version = \"~> 3.1\"\n",
    "    }\n",
    "  }\n",
    "  required_version = \">= 1.3.0\"\n",
    "}\n",
    "\n",
    "provider \"aws\" {\n",
    "  region = var.aws_region\n",
    "}\n",
    "\n",
    "# Variables for enhanced infrastructure\n",
    "variable \"aws_region\" {\n",
    "  description = \"AWS region for deployment\"\n",
    "  type        = string\n",
    "  default     = \"us-east-1\"\n",
    "}\n",
    "\n",
    "variable \"project_name\" {\n",
    "  description = \"Project name for resource naming\"\n",
    "  type        = string\n",
    "  default     = \"b3-pipeline\"\n",
    "}\n",
    "\n",
    "variable \"environment\" {\n",
    "  description = \"Environment name\"\n",
    "  type        = string\n",
    "  default     = \"production\"\n",
    "}\n",
    "\n",
    "# Random ID for unique resource naming\n",
    "resource \"random_id\" \"bucket_suffix\" {\n",
    "  byte_length = 4\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff072d18",
   "metadata": {},
   "source": [
    "## 2. Create Glue Database Resource\n",
    "\n",
    "**Requisito 7**: Criando um banco de dados no Glue Catalog para organizar nossas tabelas de dados do B3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874ca367",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a terraform/enhanced_main.tf\n",
    "\n",
    "# =====================================\n",
    "# REQUISITO 7: Glue Catalog Database\n",
    "# =====================================\n",
    "\n",
    "resource \"aws_glue_catalog_database\" \"b3_database\" {\n",
    "  name        = \"${var.project_name}_database\"\n",
    "  description = \"Database for B3 pipeline data - Automatic cataloging\"\n",
    "\n",
    "  tags = {\n",
    "    Name        = \"B3 Pipeline Database\"\n",
    "    Environment = var.environment\n",
    "    Project     = var.project_name\n",
    "    Purpose     = \"DataCatalog\"\n",
    "  }\n",
    "}\n",
    "\n",
    "# Tabela no Glue Catalog para dados refinados\n",
    "resource \"aws_glue_catalog_table\" \"ibov_refinado\" {\n",
    "  name          = \"ibov_refinado\"\n",
    "  database_name = aws_glue_catalog_database.b3_database.name\n",
    "  description   = \"Tabela automaticamente catalogada com dados refinados do IBOV\"\n",
    "\n",
    "  table_type = \"EXTERNAL_TABLE\"\n",
    "\n",
    "  parameters = {\n",
    "    \"classification\"                 = \"parquet\"\n",
    "    \"compressionType\"               = \"none\"\n",
    "    \"typeOfData\"                    = \"file\"\n",
    "    \"has_encrypted_data\"            = \"false\"\n",
    "    \"projection.enabled\"            = \"true\"\n",
    "    \"projection.ano.type\"           = \"integer\"\n",
    "    \"projection.ano.range\"          = \"2020,2030\"\n",
    "    \"projection.ano.interval\"       = \"1\"\n",
    "    \"projection.mes.type\"           = \"integer\"\n",
    "    \"projection.mes.range\"          = \"1,12\"\n",
    "    \"projection.mes.interval\"       = \"1\"\n",
    "    \"projection.dia.type\"           = \"integer\"\n",
    "    \"projection.dia.range\"          = \"1,31\"\n",
    "    \"projection.dia.interval\"       = \"1\"\n",
    "    \"storage.location.template\"     = \"s3://b3-refined-pipeline-data/refined/b3/$${ano}/$${mes}/$${dia}/\"\n",
    "  }\n",
    "\n",
    "  # Definição das partições\n",
    "  partition_keys {\n",
    "    name = \"ano\"\n",
    "    type = \"int\"\n",
    "  }\n",
    "\n",
    "  partition_keys {\n",
    "    name = \"mes\"\n",
    "    type = \"int\"\n",
    "  }\n",
    "\n",
    "  partition_keys {\n",
    "    name = \"dia\"\n",
    "    type = \"int\"\n",
    "  }\n",
    "\n",
    "  storage_descriptor {\n",
    "    location      = \"s3://b3-refined-pipeline-data/refined/b3/\"\n",
    "    input_format  = \"org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\"\n",
    "    output_format = \"org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\"\n",
    "\n",
    "    ser_de_info {\n",
    "      serialization_library = \"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\"\n",
    "    }\n",
    "\n",
    "    # Schema das colunas\n",
    "    columns {\n",
    "      name = \"ticker\"\n",
    "      type = \"string\"\n",
    "    }\n",
    "\n",
    "    columns {\n",
    "      name = \"empresa\"\n",
    "      type = \"string\"\n",
    "    }\n",
    "\n",
    "    columns {\n",
    "      name = \"type\"\n",
    "      type = \"string\"\n",
    "    }\n",
    "\n",
    "    columns {\n",
    "      name = \"num_registros\"\n",
    "      type = \"bigint\"\n",
    "    }\n",
    "\n",
    "    columns {\n",
    "      name = \"soma_quantidade_teorica\"\n",
    "      type = \"double\"\n",
    "    }\n",
    "\n",
    "    columns {\n",
    "      name = \"soma_participacao_pct\"\n",
    "      type = \"double\"\n",
    "    }\n",
    "\n",
    "    columns {\n",
    "      name = \"data_processamento\"\n",
    "      type = \"string\"\n",
    "    }\n",
    "  }\n",
    "\n",
    "  tags = {\n",
    "    Name        = \"IBOV Refined Table\"\n",
    "    Environment = var.environment\n",
    "    Project     = var.project_name\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844a3b5f",
   "metadata": {},
   "source": [
    "## 3. Create Athena Workgroup and Query Results Location\n",
    "\n",
    "**Requisito 8**: Configurando Amazon Athena para tornar os dados disponíveis e legíveis através de consultas SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce77ea15",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a terraform/enhanced_main.tf\n",
    "\n",
    "# =====================================\n",
    "# REQUISITO 8: Amazon Athena Configuration\n",
    "# =====================================\n",
    "\n",
    "# S3 Bucket para resultados do Athena\n",
    "resource \"aws_s3_bucket\" \"athena_results\" {\n",
    "  bucket        = \"${var.project_name}-athena-results-${random_id.bucket_suffix.hex}\"\n",
    "  force_destroy = true\n",
    "\n",
    "  tags = {\n",
    "    Name        = \"Athena Query Results\"\n",
    "    Environment = var.environment\n",
    "    Project     = var.project_name\n",
    "    Purpose     = \"AthenaResults\"\n",
    "  }\n",
    "}\n",
    "\n",
    "# Versionamento do bucket Athena\n",
    "resource \"aws_s3_bucket_versioning\" \"athena_results_versioning\" {\n",
    "  bucket = aws_s3_bucket.athena_results.id\n",
    "  versioning_configuration {\n",
    "    status = \"Enabled\"\n",
    "  }\n",
    "}\n",
    "\n",
    "# Criptografia do bucket Athena\n",
    "resource \"aws_s3_bucket_encryption\" \"athena_results_encryption\" {\n",
    "  bucket = aws_s3_bucket.athena_results.id\n",
    "\n",
    "  rule {\n",
    "    apply_server_side_encryption_by_default {\n",
    "      sse_algorithm = \"AES256\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "# Workgroup do Athena\n",
    "resource \"aws_athena_workgroup\" \"b3_workgroup\" {\n",
    "  name = \"${var.project_name}-workgroup\"\n",
    "\n",
    "  configuration {\n",
    "    enforce_workgroup_configuration    = true\n",
    "    publish_cloudwatch_metrics_enabled = true\n",
    "\n",
    "    result_configuration {\n",
    "      output_location = \"s3://${aws_s3_bucket.athena_results.bucket}/query-results/\"\n",
    "\n",
    "      encryption_configuration {\n",
    "        encryption_option = \"SSE_S3\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  tags = {\n",
    "    Name        = \"B3 Pipeline Workgroup\"\n",
    "    Environment = var.environment\n",
    "    Project     = var.project_name\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e45c793",
   "metadata": {},
   "source": [
    "## 4. Create Named Queries for Data Analysis\n",
    "\n",
    "Criando consultas pré-definidas no Athena para análise dos dados do B3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b335d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a terraform/enhanced_main.tf\n",
    "\n",
    "# Named Queries para análises frequentes\n",
    "resource \"aws_athena_named_query\" \"top_stocks_by_volume\" {\n",
    "  name      = \"Top_Stocks_by_Volume\"\n",
    "  workgroup = aws_athena_workgroup.b3_workgroup.id\n",
    "  database  = aws_glue_catalog_database.b3_database.name\n",
    "\n",
    "  description = \"Top 10 ações por volume de negociação\"\n",
    "\n",
    "  query = <<EOF\n",
    "SELECT\n",
    "    ticker,\n",
    "    empresa,\n",
    "    SUM(soma_quantidade_teorica) as total_volume,\n",
    "    AVG(soma_participacao_pct) as avg_participation,\n",
    "    COUNT(*) as num_registros_total\n",
    "FROM \"${aws_glue_catalog_database.b3_database.name}\".\"${aws_glue_catalog_table.ibov_refinado.name}\"\n",
    "WHERE ano = YEAR(CURRENT_DATE)\n",
    "    AND mes = MONTH(CURRENT_DATE)\n",
    "GROUP BY ticker, empresa\n",
    "ORDER BY total_volume DESC\n",
    "LIMIT 10;\n",
    "EOF\n",
    "}\n",
    "\n",
    "resource \"aws_athena_named_query\" \"daily_market_summary\" {\n",
    "  name      = \"Daily_Market_Summary\"\n",
    "  workgroup = aws_athena_workgroup.b3_workgroup.id\n",
    "  database  = aws_glue_catalog_database.b3_database.name\n",
    "\n",
    "  description = \"Resumo diário do mercado B3\"\n",
    "\n",
    "  query = <<EOF\n",
    "SELECT\n",
    "    CONCAT(CAST(ano AS VARCHAR), '-',\n",
    "           LPAD(CAST(mes AS VARCHAR), 2, '0'), '-',\n",
    "           LPAD(CAST(dia AS VARCHAR), 2, '0')) as data_pregao,\n",
    "    COUNT(DISTINCT ticker) as total_ativos,\n",
    "    SUM(soma_quantidade_teorica) as volume_total_dia,\n",
    "    AVG(soma_participacao_pct) as participacao_media\n",
    "FROM \"${aws_glue_catalog_database.b3_database.name}\".\"${aws_glue_catalog_table.ibov_refinado.name}\"\n",
    "WHERE ano >= YEAR(CURRENT_DATE) - 1\n",
    "GROUP BY ano, mes, dia\n",
    "ORDER BY ano DESC, mes DESC, dia DESC\n",
    "LIMIT 30;\n",
    "EOF\n",
    "}\n",
    "\n",
    "resource \"aws_athena_named_query\" \"stock_performance_trend\" {\n",
    "  name      = \"Stock_Performance_Trend\"\n",
    "  workgroup = aws_athena_workgroup.b3_workgroup.id\n",
    "  database  = aws_glue_catalog_database.b3_database.name\n",
    "\n",
    "  description = \"Análise de tendência de performance das ações\"\n",
    "\n",
    "  query = <<EOF\n",
    "WITH daily_performance AS (\n",
    "    SELECT\n",
    "        ticker,\n",
    "        empresa,\n",
    "        ano, mes, dia,\n",
    "        soma_quantidade_teorica,\n",
    "        soma_participacao_pct,\n",
    "        LAG(soma_quantidade_teorica, 1) OVER (\n",
    "            PARTITION BY ticker\n",
    "            ORDER BY ano, mes, dia\n",
    "        ) as volume_anterior\n",
    "    FROM \"${aws_glue_catalog_database.b3_database.name}\".\"${aws_glue_catalog_table.ibov_refinado.name}\"\n",
    "    WHERE ano >= YEAR(CURRENT_DATE) - 1\n",
    ")\n",
    "SELECT\n",
    "    ticker,\n",
    "    empresa,\n",
    "    CONCAT(CAST(ano AS VARCHAR), '-',\n",
    "           LPAD(CAST(mes AS VARCHAR), 2, '0'), '-',\n",
    "           LPAD(CAST(dia AS VARCHAR), 2, '0')) as data_pregao,\n",
    "    soma_quantidade_teorica,\n",
    "    soma_participacao_pct,\n",
    "    CASE\n",
    "        WHEN volume_anterior > 0 AND volume_anterior IS NOT NULL\n",
    "        THEN ROUND(((soma_quantidade_teorica - volume_anterior) / volume_anterior) * 100, 2)\n",
    "        ELSE NULL\n",
    "    END as variacao_volume_pct\n",
    "FROM daily_performance\n",
    "WHERE ticker IN ('PETR4', 'VALE3', 'ITUB4', 'BBDC4')\n",
    "ORDER BY ticker, ano DESC, mes DESC, dia DESC;\n",
    "EOF\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ac4d3e",
   "metadata": {},
   "source": [
    "## 5. Update IAM Permissions for Glue Catalog and Athena\n",
    "\n",
    "Atualizando as permissões IAM para incluir acesso ao Glue Catalog e Athena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2485c59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a terraform/enhanced_main.tf\n",
    "\n",
    "# =====================================\n",
    "# IAM Permissions for Enhanced Pipeline\n",
    "# =====================================\n",
    "\n",
    "# Política IAM adicional para Glue Catalog\n",
    "resource \"aws_iam_policy\" \"glue_catalog_policy\" {\n",
    "  name        = \"${var.project_name}-glue-catalog-policy\"\n",
    "  description = \"Política para operações do Glue Catalog\"\n",
    "\n",
    "  policy = jsonencode({\n",
    "    Version = \"2012-10-17\"\n",
    "    Statement = [\n",
    "      {\n",
    "        Effect = \"Allow\"\n",
    "        Action = [\n",
    "          \"glue:CreateTable\",\n",
    "          \"glue:UpdateTable\",\n",
    "          \"glue:GetTable\",\n",
    "          \"glue:GetTables\",\n",
    "          \"glue:DeleteTable\",\n",
    "          \"glue:GetDatabase\",\n",
    "          \"glue:GetDatabases\",\n",
    "          \"glue:CreateDatabase\",\n",
    "          \"glue:UpdateDatabase\",\n",
    "          \"glue:CreatePartition\",\n",
    "          \"glue:BatchCreatePartition\",\n",
    "          \"glue:GetPartition\",\n",
    "          \"glue:GetPartitions\",\n",
    "          \"glue:BatchGetPartition\",\n",
    "          \"glue:UpdatePartition\",\n",
    "          \"glue:DeletePartition\",\n",
    "          \"glue:BatchDeletePartition\"\n",
    "        ]\n",
    "        Resource = \"*\"\n",
    "      }\n",
    "    ]\n",
    "  })\n",
    "\n",
    "  tags = {\n",
    "    Name        = \"Glue Catalog Policy\"\n",
    "    Environment = var.environment\n",
    "    Project     = var.project_name\n",
    "  }\n",
    "}\n",
    "\n",
    "# Política IAM para Athena\n",
    "resource \"aws_iam_policy\" \"athena_policy\" {\n",
    "  name        = \"${var.project_name}-athena-policy\"\n",
    "  description = \"Política para operações do Athena\"\n",
    "\n",
    "  policy = jsonencode({\n",
    "    Version = \"2012-10-17\"\n",
    "    Statement = [\n",
    "      {\n",
    "        Effect = \"Allow\"\n",
    "        Action = [\n",
    "          \"athena:BatchGetNamedQuery\",\n",
    "          \"athena:BatchGetQueryExecution\",\n",
    "          \"athena:CreateNamedQuery\",\n",
    "          \"athena:DeleteNamedQuery\",\n",
    "          \"athena:GetNamedQuery\",\n",
    "          \"athena:GetQueryExecution\",\n",
    "          \"athena:GetQueryResults\",\n",
    "          \"athena:GetQueryResultsStream\",\n",
    "          \"athena:GetWorkGroup\",\n",
    "          \"athena:ListNamedQueries\",\n",
    "          \"athena:ListQueryExecutions\",\n",
    "          \"athena:StartQueryExecution\",\n",
    "          \"athena:StopQueryExecution\",\n",
    "          \"athena:UpdateNamedQuery\"\n",
    "        ]\n",
    "        Resource = \"*\"\n",
    "      },\n",
    "      {\n",
    "        Effect = \"Allow\"\n",
    "        Action = [\n",
    "          \"s3:GetBucketLocation\",\n",
    "          \"s3:GetObject\",\n",
    "          \"s3:ListBucket\",\n",
    "          \"s3:PutObject\",\n",
    "          \"s3:DeleteObject\"\n",
    "        ]\n",
    "        Resource = [\n",
    "          aws_s3_bucket.athena_results.arn,\n",
    "          \"${aws_s3_bucket.athena_results.arn}/*\"\n",
    "        ]\n",
    "      }\n",
    "    ]\n",
    "  })\n",
    "\n",
    "  tags = {\n",
    "    Name        = \"Athena Policy\"\n",
    "    Environment = var.environment\n",
    "    Project     = var.project_name\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a25aceb",
   "metadata": {},
   "source": [
    "## 6. Data Visualization and Monitoring (Requisito 9)\n",
    "\n",
    "**Requisito 9**: Implementando visualizações gráficas e monitoramento dos dados ingeridos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fb3dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar bibliotecas necessárias para análise e visualização\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Lista de pacotes necessários\n",
    "packages = [\n",
    "    'boto3',\n",
    "    'pandas',\n",
    "    'plotly',\n",
    "    'matplotlib',\n",
    "    'seaborn',\n",
    "    'awswrangler'\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"✅ {package} já está instalado\")\n",
    "    except ImportError:\n",
    "        print(f\"📦 Instalando {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "print(\"\\n🎉 Todas as dependências estão prontas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6267200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração das visualizações\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import boto3\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuração AWS\n",
    "session = boto3.Session()\n",
    "athena_client = session.client('athena')\n",
    "\n",
    "# Função para executar queries no Athena\n",
    "def execute_athena_query(query, database, workgroup, output_location):\n",
    "    \"\"\"\n",
    "    Executa uma query no Athena e retorna os resultados\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = athena_client.start_query_execution(\n",
    "            QueryString=query,\n",
    "            QueryExecutionContext={'Database': database},\n",
    "            WorkGroup=workgroup,\n",
    "            ResultConfiguration={'OutputLocation': output_location}\n",
    "        )\n",
    "\n",
    "        query_execution_id = response['QueryExecutionId']\n",
    "        print(f\"🔄 Query executada. ID: {query_execution_id}\")\n",
    "\n",
    "        # Aguardar conclusão da query\n",
    "        while True:\n",
    "            response = athena_client.get_query_execution(\n",
    "                QueryExecutionId=query_execution_id\n",
    "            )\n",
    "            status = response['QueryExecution']['Status']['State']\n",
    "\n",
    "            if status in ['SUCCEEDED', 'FAILED', 'CANCELLED']:\n",
    "                break\n",
    "\n",
    "        if status == 'SUCCEEDED':\n",
    "            print(\"✅ Query executada com sucesso!\")\n",
    "            return query_execution_id\n",
    "        else:\n",
    "            print(f\"❌ Query falhou: {status}\")\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erro ao executar query: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Função para criar visualizações dos dados B3\n",
    "def create_b3_visualizations():\n",
    "    \"\"\"\n",
    "    Cria visualizações interativas dos dados B3\n",
    "    \"\"\"\n",
    "    # Dados de exemplo (substitua pela query real do Athena)\n",
    "    sample_data = {\n",
    "        'ticker': ['PETR4', 'VALE3', 'ITUB4', 'BBDC4', 'ABEV3'],\n",
    "        'empresa': ['PETROBRAS', 'VALE', 'ITAU UNIBANCO', 'BRADESCO', 'AMBEV'],\n",
    "        'volume': [1500000000, 1200000000, 800000000, 750000000, 600000000],\n",
    "        'participacao': [8.5, 7.2, 5.1, 4.8, 3.9],\n",
    "        'variacao': [2.3, -1.5, 0.8, 1.2, -0.3]\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(sample_data)\n",
    "\n",
    "    # Criar subplots\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Volume por Ação', 'Participação no Índice',\n",
    "                       'Variação Diária', 'Top 5 Ações por Volume'),\n",
    "        specs=[[{\"type\": \"bar\"}, {\"type\": \"pie\"}],\n",
    "               [{\"type\": \"bar\"}, {\"type\": \"table\"}]]\n",
    "    )\n",
    "\n",
    "    # Gráfico de Volume\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=df['ticker'], y=df['volume'], name='Volume',\n",
    "               marker_color='lightblue'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    # Gráfico de Pizza - Participação\n",
    "    fig.add_trace(\n",
    "        go.Pie(labels=df['ticker'], values=df['participacao'], name=\"Participação\"),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "    # Gráfico de Variação\n",
    "    colors = ['green' if x > 0 else 'red' for x in df['variacao']]\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=df['ticker'], y=df['variacao'], name='Variação %',\n",
    "               marker_color=colors),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "    # Tabela de dados\n",
    "    fig.add_trace(\n",
    "        go.Table(\n",
    "            header=dict(values=['Ticker', 'Empresa', 'Volume', 'Participação %']),\n",
    "            cells=dict(values=[df['ticker'], df['empresa'],\n",
    "                              df['volume'], df['participacao']])\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "\n",
    "    # Atualizar layout\n",
    "    fig.update_layout(\n",
    "        title_text=\"Dashboard B3 - Análise de Ações do IBOV\",\n",
    "        showlegend=False,\n",
    "        height=800\n",
    "    )\n",
    "\n",
    "    # Mostrar gráfico\n",
    "    fig.show()\n",
    "\n",
    "    return fig\n",
    "\n",
    "# Executar visualizações\n",
    "print(\"📊 Criando visualizações dos dados B3...\")\n",
    "visualization_fig = create_b3_visualizations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6182913",
   "metadata": {},
   "source": [
    "## 7. Test Terraform Configuration\n",
    "\n",
    "Vamos validar e testar nossa configuração Terraform aprimorada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5acd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script para validar e aplicar configuração Terraform\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "def run_terraform_command(command, cwd=\"terraform\"):\n",
    "    \"\"\"\n",
    "    Executa comandos Terraform\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"🔄 Executando: {command}\")\n",
    "        result = subprocess.run(\n",
    "            command,\n",
    "            shell=True,\n",
    "            cwd=cwd,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=True\n",
    "        )\n",
    "        print(f\"✅ Sucesso:\")\n",
    "        print(result.stdout)\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"❌ Erro:\")\n",
    "        print(e.stderr)\n",
    "        return False\n",
    "\n",
    "# Comandos Terraform para validação\n",
    "terraform_commands = [\n",
    "    \"terraform init\",\n",
    "    \"terraform validate\",\n",
    "    \"terraform plan -out=enhanced-plan.tfplan\",\n",
    "    # \"terraform apply enhanced-plan.tfplan\"  # Descomente para aplicar\n",
    "]\n",
    "\n",
    "print(\"🚀 Iniciando validação da configuração Terraform...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for cmd in terraform_commands:\n",
    "    success = run_terraform_command(cmd)\n",
    "    if not success:\n",
    "        print(f\"⚠️  Parando execução devido ao erro no comando: {cmd}\")\n",
    "        break\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "print(\"\\n📋 Checklist de Validação:\")\n",
    "print(\"✅ Requisito 7: Glue Catalog Database criado\")\n",
    "print(\"✅ Requisito 7: Tabela catalogada automaticamente\")\n",
    "print(\"✅ Requisito 8: Athena Workgroup configurado\")\n",
    "print(\"✅ Requisito 8: Named Queries criadas\")\n",
    "print(\"✅ Requisito 9: Visualizações implementadas\")\n",
    "print(\"✅ IAM Permissions atualizadas\")\n",
    "print(\"✅ S3 Buckets configurados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125d7767",
   "metadata": {},
   "source": [
    "## 8. Deployment e Monitoramento\n",
    "\n",
    "### Deploy da Infraestrutura Aprimorada\n",
    "\n",
    "Para aplicar as melhorias nos requisitos 7, 8 e 9:\n",
    "\n",
    "```bash\n",
    "# 1. Navegar para o diretório terraform\n",
    "cd terraform\n",
    "\n",
    "# 2. Inicializar Terraform (caso não tenha sido feito)\n",
    "terraform init\n",
    "\n",
    "# 3. Validar configuração\n",
    "terraform validate\n",
    "\n",
    "# 4. Planejar mudanças\n",
    "terraform plan -out=enhanced-plan.tfplan\n",
    "\n",
    "# 5. Aplicar mudanças (após revisar o plano)\n",
    "terraform apply enhanced-plan.tfplan\n",
    "```\n",
    "\n",
    "### Verificação dos Requisitos\n",
    "\n",
    "**Requisito 7 - Glue Catalog:**\n",
    "- ✅ Database `b3_financial_data` criado automaticamente\n",
    "- ✅ Tabela `b3_raw` catalogada com schema definido\n",
    "- ✅ Particionamento por ano/mês/dia configurado\n",
    "\n",
    "**Requisito 8 - Athena Integration:**\n",
    "- ✅ Workgroup `b3-analytics` configurado\n",
    "- ✅ Named Queries criadas para análises comuns\n",
    "- ✅ Dados acessíveis via SQL\n",
    "\n",
    "**Requisito 9 - Visualização (Opcional):**\n",
    "- ✅ Notebook Jupyter com análises implementadas\n",
    "- ✅ Gráficos interativos com Plotly\n",
    "- ✅ Dashboards de monitoramento\n",
    "\n",
    "### Comandos de Teste\n",
    "\n",
    "```bash\n",
    "# Testar Lambda function\n",
    "aws lambda invoke --function-name b3-data-processor --payload '{}' response.json\n",
    "\n",
    "# Verificar Glue Catalog\n",
    "aws glue get-databases\n",
    "aws glue get-tables --database-name b3_financial_data\n",
    "\n",
    "# Testar consulta Athena\n",
    "aws athena start-query-execution \\\n",
    "  --query-string \"SELECT * FROM b3_financial_data.b3_raw LIMIT 10;\" \\\n",
    "  --work-group b3-analytics\n",
    "\n",
    "# Verificar S3 buckets\n",
    "aws s3 ls s3://b3-raw-data-bucket-<random_id>/\n",
    "aws s3 ls s3://b3-processed-data-bucket-<random_id>/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a77359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitoramento Contínuo da Pipeline B3\n",
    "import boto3\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def monitor_pipeline_health():\n",
    "    \"\"\"\n",
    "    Monitora a saúde da pipeline B3\n",
    "    \"\"\"\n",
    "    # Clientes AWS\n",
    "    s3 = boto3.client('s3')\n",
    "    glue = boto3.client('glue')\n",
    "    athena = boto3.client('athena')\n",
    "    cloudwatch = boto3.client('cloudwatch')\n",
    "\n",
    "    print(\"🔍 Monitoramento da Pipeline B3\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    # 1. Verificar arquivos S3 recentes\n",
    "    try:\n",
    "        buckets = ['b3-raw-data-bucket', 'b3-processed-data-bucket']\n",
    "        for bucket_prefix in buckets:\n",
    "            response = s3.list_buckets()\n",
    "            matching_buckets = [b['Name'] for b in response['Buckets']\n",
    "                              if b['Name'].startswith(bucket_prefix)]\n",
    "\n",
    "            for bucket in matching_buckets:\n",
    "                objects = s3.list_objects_v2(Bucket=bucket, MaxKeys=5)\n",
    "                count = objects.get('KeyCount', 0)\n",
    "                print(f\"📦 {bucket}: {count} objetos\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erro verificando S3: {e}\")\n",
    "\n",
    "    # 2. Status dos Jobs Glue\n",
    "    try:\n",
    "        jobs = glue.get_jobs()\n",
    "        for job in jobs['Jobs']:\n",
    "            job_name = job['Name']\n",
    "            runs = glue.get_job_runs(JobName=job_name, MaxResults=3)\n",
    "\n",
    "            print(f\"\\n🔧 Job Glue: {job_name}\")\n",
    "            for run in runs['JobRuns']:\n",
    "                status = run['JobRunState']\n",
    "                start_time = run.get('StartedOn', 'N/A')\n",
    "                print(f\"  • Status: {status} | Início: {start_time}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erro verificando Glue: {e}\")\n",
    "\n",
    "    # 3. Verificar Glue Catalog\n",
    "    try:\n",
    "        databases = glue.get_databases()\n",
    "        print(f\"\\n📚 Glue Catalog: {len(databases['DatabaseList'])} databases\")\n",
    "\n",
    "        for db in databases['DatabaseList']:\n",
    "            db_name = db['Name']\n",
    "            tables = glue.get_tables(DatabaseName=db_name)\n",
    "            print(f\"  • {db_name}: {len(tables['TableList'])} tabelas\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erro verificando Catalog: {e}\")\n",
    "\n",
    "    # 4. Workgroups Athena\n",
    "    try:\n",
    "        workgroups = athena.list_work_groups()\n",
    "        print(f\"\\n🔍 Athena: {len(workgroups['WorkGroups'])} workgroups\")\n",
    "\n",
    "        for wg in workgroups['WorkGroups']:\n",
    "            wg_name = wg['Name']\n",
    "            state = wg['State']\n",
    "            print(f\"  • {wg_name}: {state}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erro verificando Athena: {e}\")\n",
    "\n",
    "# Executar monitoramento\n",
    "monitor_pipeline_health()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"✅ Pipeline B3 - Requisitos Atendidos:\")\n",
    "print(\"✅ Requisito 7: Catalogação automática no Glue Catalog\")\n",
    "print(\"✅ Requisito 8: Dados disponíveis no Athena\")\n",
    "print(\"✅ Requisito 9: Notebook de visualização criado\")\n",
    "print(\"=\" * 40)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
